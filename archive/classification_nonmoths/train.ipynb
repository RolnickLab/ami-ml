{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor       : Aditya Jain\\nDate Started : May 2, 2022\\nAbout        : This is the main training file for training the binary classifier, moths non-moths\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author       : Aditya Jain\n",
    "Date Started : May 2, 2022\n",
    "About        : This is the main training file for training the binary classifier, moths non-moths\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision.models as torchmodels\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from models.resnet50 import Resnet50\n",
    "from data.nonmothdataset import NonMothDataset\n",
    "from training_params.loss import Loss\n",
    "from training_params.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoth-ai\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mila/a/aditya.jain/mothAI/classification_nonmoths/wandb/run-20220505_110415-2ang2pzi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/moth-ai/nonmoth-classifier/runs/2ang2pzi\" target=\"_blank\">light-parsec-4</a></strong> to <a href=\"https://wandb.ai/moth-ai/nonmoth-classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/moth-ai/nonmoth-classifier/runs/2ang2pzi?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbf0610ec10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file   = 'config/01-config.json' #**** This file path should come through command line argument\n",
    "\n",
    "f             = open(config_file)\n",
    "config_data   = json.load(f)\n",
    "# print(json.dumps(config_data, indent=3))\n",
    "\n",
    "image_resize  = config_data['training']['image_resize']\n",
    "root_dir      = config_data['dataset']['root_dir']\n",
    "train_set     = config_data['dataset']['train_set']\n",
    "val_set       = config_data['dataset']['val_set']\n",
    "test_set      = config_data['dataset']['test_set']\n",
    "batch_size    = config_data['training']['batch_size']\n",
    "epochs        = config_data['training']['epochs']\n",
    "loss_name     = config_data['training']['loss']['name']\n",
    "early_stop    = config_data['training']['early_stopping']\n",
    "start_val_los = config_data['training']['start_val_loss']\n",
    "\n",
    "no_classes    = config_data['model']['num_classes']\n",
    "\n",
    "opt_name      = config_data['training']['optimizer']['name']\n",
    "learning_rate = config_data['training']['optimizer']['learning_rate']\n",
    "momentum      = config_data['training']['optimizer']['momentum']\n",
    "\n",
    "mod_save_pth  = config_data['training']['model_save_path']\n",
    "mod_name      = config_data['training']['model_name']\n",
    "mod_ver       = config_data['training']['version']\n",
    "DTSTR         = datetime.datetime.now()\n",
    "DTSTR         = DTSTR.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "save_path     = mod_save_pth + mod_ver + '_' + mod_name + '_' + DTSTR + '.pt'\n",
    "\n",
    "wandb.init(project=\"nonmoth-classifier\", entity=\"moth-ai\", config=config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model = Resnet50(config_data).to(device)\n",
    "# print(model)\n",
    "# print(summary(model, (3,224,224)))  # keras-type model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data loader\n",
    "train_transformer = transforms.Compose([\n",
    "                        transforms.Resize((image_resize, image_resize)),              # resize the image to 224x224 \n",
    "                        transforms.RandomVerticalFlip(0.3),\n",
    "                        transforms.ToTensor()])\n",
    "train_data        = NonMothDataset(root_dir, train_set, train_transformer)\n",
    "train_dataloader  = DataLoader(train_data,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# validation data loader\n",
    "val_transformer   = transforms.Compose([\n",
    "                        transforms.Resize((image_resize, image_resize)),              # resize the image to 224x224 \n",
    "                        transforms.ToTensor()])\n",
    "val_data          = NonMothDataset(root_dir, val_set, val_transformer)\n",
    "val_dataloader    = DataLoader(val_data,batch_size=batch_size)\n",
    "\n",
    "# test data loader\n",
    "test_transformer  = transforms.Compose([\n",
    "                        transforms.Resize((image_resize, image_resize)),              # resize the image to 224x224 \n",
    "                        transforms.ToTensor()])\n",
    "test_data         = NonMothDataset(root_dir, test_set, test_transformer)\n",
    "test_dataloader   = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = Loss(loss_name).func()\n",
    "optimizer = Optimizer(opt_name, model, learning_rate, momentum).func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(prediction, label):\n",
    "    \"\"\"\n",
    "    returns the total and correct predictions for a batch\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction     = F.softmax(prediction, dim=1)\n",
    "    prediction_idx = torch.argmax(prediction, dim=1)\n",
    "    corr_pred      = 0\n",
    "    \n",
    "    for i in range(len(label)):\n",
    "        if label[i]==prediction_idx[i]:\n",
    "            corr_pred += 1\n",
    "    \n",
    "    return len(label), corr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '/home/mila/a/aditya.jain/scratch/GBIF_Data/moths/Noctuidae/Elaphria/Elaphria alapallida/1846822794.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d04a7f9667e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_corr_pred\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m# switching model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlabel_batch\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/milamoth/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/milamoth/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/milamoth/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/milamoth/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mothAI/classification_nonmoths/data/nonmothdataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mlabel\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mimage\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mimage\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/milamoth/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2966\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2967\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2968\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2969\u001b[0m     )\n\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/home/mila/a/aditya.jain/scratch/GBIF_Data/moths/Noctuidae/Elaphria/Elaphria alapallida/1846822794.jpg'"
     ]
    }
   ],
   "source": [
    "lowest_val_loss = start_val_los\n",
    "early_stp_count = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    val_loss   = 0\n",
    "    s_time     = time.time()\n",
    "\n",
    "    # model training on training dataset\n",
    "    train_total_pred = 0\n",
    "    train_corr_pred  = 0\n",
    "    model.train()                      # switching model to training mode\n",
    "    for image_batch, label_batch in train_dataloader:    \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        label_batch              = label_batch.squeeze_()          \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs   = model(image_batch)        \n",
    "        t_loss    = loss_func(outputs, label_batch)\n",
    "        t_loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += t_loss.item()\n",
    "        \n",
    "        # accuracy evaluation\n",
    "        cur_total_pred, cur_corr_pred = calculate_accuracy(outputs, label_batch)\n",
    "        train_total_pred += cur_total_pred\n",
    "        train_corr_pred  += cur_corr_pred\n",
    "        \n",
    "#         print(train_total_pred, train_corr_pred)\n",
    "#         break\n",
    "        \n",
    "    # model evaluation on validation dataset\n",
    "    val_total_pred = 0\n",
    "    val_corr_pred  = 0\n",
    "    model.eval()                       # switching model to evaluation mode\n",
    "    for image_batch, label_batch in val_dataloader:\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        label_batch              = label_batch.squeeze_()        \n",
    "        \n",
    "        outputs   = model(image_batch)        \n",
    "        v_loss    = loss_func(outputs, label_batch)\n",
    "        val_loss += v_loss.item()    \n",
    "        \n",
    "        # accuracy evaluation\n",
    "        cur_total_pred, cur_corr_pred = calculate_accuracy(outputs, label_batch)\n",
    "        val_total_pred += cur_total_pred\n",
    "        val_corr_pred  += cur_corr_pred\n",
    "    \n",
    "    if val_loss<lowest_val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss':val_loss}, \n",
    "            save_path)               \n",
    "        lowest_val_loss = val_loss\n",
    "        early_stp_count = 0\n",
    "    else:\n",
    "        early_stp_count += 1        \n",
    "\n",
    "    \n",
    "    # logging metrics\n",
    "    wandb.log({'training loss': train_loss, 'validation loss': val_loss})\n",
    "    wandb.log({'training accuracy': (train_corr_pred/train_total_pred)*100,\\\n",
    "               'validation accuracy': (val_corr_pred/val_total_pred)*100})  \n",
    "    e_time = (time.time()-s_time)/60   # time taken in minutes   \n",
    "    wandb.log({'time per epoch': e_time})\n",
    "    \n",
    "    if early_stp_count >= early_stop:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = save_path\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.load_state_dict(torch.load('logs/mothmodel_2021-05-18-07-36.pt', map_location=device))   #**** Wouldn't need this while running this as a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()       \n",
    "\n",
    "print(\"Prediction on test data started ...\")\n",
    "\n",
    "test_total_pred = 0\n",
    "test_corr_pred  = 0\n",
    "with torch.no_grad():                                 # switching off gradient computation in evaluation mode\n",
    "    for image_batch, label_batch in test_dataloader:  \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        predictions              = model(image_batch)\n",
    "        \n",
    "        # accuracy evaluation\n",
    "        cur_total_pred, cur_corr_pred = calculate_accuracy(predictions, label_batch)\n",
    "        test_total_pred += cur_total_pred\n",
    "        test_corr_pred  += cur_corr_pred\n",
    "        \n",
    "        wandb.log({'test accuracy': (test_corr_pred/test_total_pred)*100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = '/home/mila/a/aditya.jain/scratch/GBIF_Data/moths/Noctuidae/Elaphria/Elaphria alapallida/1846822794.jpg'\n",
    "image = cv2.imread(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1464e44aa5be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2682\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2683\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2684\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2685\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5669\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5671\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5672\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    683\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m    684\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0;32m--> 685\u001b[0;31m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "if image is None:\n",
    "    print('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth)",
   "language": "python",
   "name": "milamoth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
