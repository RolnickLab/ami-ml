{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author        : Aditya Jain\n",
    "Date started  : 9th September, 2021\n",
    "About         : This script is used for the self-supervision idea\n",
    "'''\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "resnet_mod = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video(video_path, des_fps, img_size):\n",
    "    ''' \n",
    "    Given a video, prepares the final downsized concatenated image for training\n",
    "    Arguments:\n",
    "    video_path: the file path of video\n",
    "    n_frames  : no of frames to sample from the video at equal intervals\n",
    "    down_f    : downscaling factor for the image\n",
    "    '''\n",
    "    \n",
    "    vidcap      = cv2.VideoCapture(video_path)     \n",
    "    fps         = vidcap.get(cv2.CAP_PROP_FPS)           #  FPS of the video      \n",
    "    frame_count = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)   #  total frame count\n",
    "    total_sec   = frame_count/fps\n",
    "    sec         = 0\n",
    "    n_frames    = total_sec*des_fps\n",
    "    time_sec    = total_sec/n_frames                     # the video will be sampled after every time_sec\n",
    "    final_img   = []\n",
    "        \n",
    "    while sec < total_sec:        \n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)    # setting which frame to get        \n",
    "        success, image = vidcap.read()\n",
    "\n",
    "        if success:\n",
    "            transformer = transforms.Compose([\n",
    "                            transforms.Resize((img_size, img_size)),              # resize the image to 224x224 \n",
    "                            transforms.ToTensor()])\n",
    "            image       = Image.fromarray(image)\n",
    "            image       = transformer(image)\n",
    "            \n",
    "            final_img.append(image)\n",
    "            \n",
    "        sec += time_sec        \n",
    "        \n",
    "    return torch.Tensor(final_img.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-8860c3be49aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mframe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_fps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-3eef22882754>\u001b[0m in \u001b[0;36mframes_from_video\u001b[0;34m(video_path, des_fps, img_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "img_size   = 224\n",
    "vid_path   = 'dog.mp4'\n",
    "target_fps = 5\n",
    "\n",
    "frame_list = frames_from_video(vid_path, target_fps, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torch.unsqueeze(frame_list[0], 0).to(device)\n",
    "prediction = resnet_mod(test_image)\n",
    "values, predict_indx = torch.topk(prediction, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "newenv"
  },
  "kernelspec": {
   "display_name": "milamoth",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
