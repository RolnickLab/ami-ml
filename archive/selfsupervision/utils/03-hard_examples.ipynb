{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author        : Aditya Jain\n",
    "Date started  : 8th November, 2021\n",
    "About         : This script is used for finding the hard examples using probability density\n",
    "'''\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pre-trained ImageNet Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resnet_mod = models.resnet50(pretrained=True)\n",
    "resnet_mod = resnet_mod.to(device)\n",
    "resnet_mod.eval()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading ImageNet Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(image, model, transformer, categories, device):\n",
    "    '''\n",
    "    returns the ImageNet class label for a model's prediction on an image\n",
    "    '''\n",
    "    softmax               = nn.Softmax(dim=1)\n",
    "    \n",
    "    image                 = Image.fromarray(image)\n",
    "    image                 = transformer(image)            \n",
    "    image                 = torch.unsqueeze(image, 0).to(device)\n",
    "    \n",
    "    prediction            = model(image)\n",
    "    pred_softmax          = softmax(prediction)\n",
    "    pred_val, pred_indx   = torch.topk(pred_softmax, 1)\n",
    "    index                 = pred_indx.detach().cpu().numpy()\n",
    "    \n",
    "    return pred_val.detach().cpu().numpy(), categories[int(index[0])]\n",
    "\n",
    "\n",
    "def save_logic(prediction_list, cur_class, threshold):\n",
    "    '''\n",
    "    implements the logic if the current frame should be classified as a hard example\n",
    "    Args:\n",
    "        prediction_list (list)  : the list of window predictions, contains class and confidence\n",
    "        cur_class (string)      : the label of the current frame\n",
    "        threshold (float)       : min. threshold for the most popular class \n",
    "        \n",
    "        returns (bool)     : if current frame is a hard example\n",
    "    '''\n",
    "    tot_items   = len(prediction_list)\n",
    "    mid_elem    = tot_items//2\n",
    "    other_class = prediction_list[0][0]    # classes apart from the central frame\n",
    "    \n",
    "    for i in range(tot_items):\n",
    "        if i!=mid_elem:\n",
    "            if prediction_list[i][0]!=other_class or prediction_list[i][1]<threshold:\n",
    "                return False, ''\n",
    "    \n",
    "    if prediction_list[mid_elem][0]!=other_class:\n",
    "        return True, other_class\n",
    "    else:\n",
    "        return False, ''\n",
    "    \n",
    "\n",
    "def hard_examples(video_path, model, save_loc, window, threshold, categories, device):\n",
    "    '''\n",
    "    given an input video, finds and saves the hard examples\n",
    "    Args:\n",
    "        video_path (string): path for the video to be evaluated\n",
    "        model (torch model): model to be ran for evaluation\n",
    "        save_loc (string)  : location for the saving of hard examples\n",
    "        window (int)       : no of frames to check on either side\n",
    "        \n",
    "        returns            : saves hard examples and count of examples in a video\n",
    "    '''\n",
    "    softmax        = nn.Softmax(dim=1)\n",
    "    img_size       = 224    \n",
    "    transformer    = transforms.Compose([\n",
    "                            transforms.Resize((img_size, img_size)),                            \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    vidcap      = cv2.VideoCapture(video_path)     \n",
    "    fps         = vidcap.get(cv2.CAP_PROP_FPS)           #  FPS of the video \n",
    "    frame_count = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)   #  total frame count \n",
    "    frame_indx  = window                                 #  starts from the window offset\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_list  = []\n",
    "        while frame_indx < (frame_count-window):\n",
    "            cur_class   = ''\n",
    "            cur_image  = ''\n",
    "            \n",
    "            if pred_list==[]:\n",
    "                for frame in range(frame_indx-window, frame_indx+window+1):           \n",
    "                    vidcap.set(cv2.CAP_PROP_POS_FRAMES, frame)    # setting which frame to get        \n",
    "                    success, image = vidcap.read()\n",
    "                \n",
    "                    if success:\n",
    "                        p_probab, p_class  = model_prediction(image, model, transformer, categories, device) \n",
    "                        pred_list.append([p_class, p_probab[0][0]])\n",
    "                        \n",
    "                        # getting the label and image for current frame    \n",
    "                        if frame==frame_indx:\n",
    "                            cur_class = p_class\n",
    "                            cur_image = image               \n",
    "            else:\n",
    "                cur_elem  = len(pred_list)//2 + 1\n",
    "                cur_class = pred_list[cur_elem][0]\n",
    "                \n",
    "                vidcap.set(cv2.CAP_PROP_POS_FRAMES, frame_indx)    # setting which frame to get        \n",
    "                success, image = vidcap.read()                \n",
    "                if success:\n",
    "                    cur_image = image\n",
    "                 \n",
    "                pred_list      = pred_list[1:]     # don't need the first element now\n",
    "                vidcap.set(cv2.CAP_PROP_POS_FRAMES, frame_indx+window)    # setting which frame to get        \n",
    "                success, image = vidcap.read()                \n",
    "                if success:\n",
    "                    p_probab, p_class  = model_prediction(image, model, transformer, categories, device)\n",
    "                    pred_list.append([p_class, p_probab[0][0]])                \n",
    "            \n",
    "            flag, correct_class = save_logic(pred_list, cur_class, threshold)           \n",
    "            if flag:\n",
    "                save_dir = save_loc + correct_class\n",
    "                \n",
    "                # making a directory; if needed\n",
    "                try:    \n",
    "                    os.makedirs(save_dir)                     \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                exist_count   = len(os.listdir(save_dir))    # count of existing files in the folder\n",
    "                img_save_path = save_dir + '/' + str(exist_count+1) + '_' + cur_class + '.jpg'\n",
    "                cv2.imwrite(img_save_path, cur_image)\n",
    "                print('Found hard example: ', img_save_paths)\n",
    "                \n",
    "            frame_indx += 1                  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mila/a/aditya.jain/scratch/selfsupervise_data/natgeo_wild/natgeo_wild_6.mp4\n",
      "[['matchstick', 0.034310054], ['matchstick', 0.042360384], ['matchstick', 0.042277977], ['volcano', 0.085664585], ['volcano', 0.0777667]]\n",
      "[['matchstick', 0.042360384], ['matchstick', 0.042277977], ['volcano', 0.085664585], ['volcano', 0.0777667], ['volcano', 0.11449845]]\n",
      "[['matchstick', 0.042277977], ['volcano', 0.085664585], ['volcano', 0.0777667], ['volcano', 0.11449845], ['volcano', 0.111469276]]\n",
      "[['volcano', 0.085664585], ['volcano', 0.0777667], ['volcano', 0.11449845], ['volcano', 0.111469276], ['promontory', 0.2359575]]\n",
      "[['volcano', 0.0777667], ['volcano', 0.11449845], ['volcano', 0.111469276], ['promontory', 0.2359575], ['promontory', 0.2791044]]\n",
      "[['volcano', 0.11449845], ['volcano', 0.111469276], ['promontory', 0.2359575], ['promontory', 0.2791044], ['promontory', 0.21200985]]\n",
      "[['volcano', 0.111469276], ['promontory', 0.2359575], ['promontory', 0.2791044], ['promontory', 0.21200985], ['promontory', 0.23786893]]\n",
      "[['promontory', 0.2359575], ['promontory', 0.2791044], ['promontory', 0.21200985], ['promontory', 0.23786893], ['promontory', 0.23685344]]\n",
      "[['promontory', 0.2791044], ['promontory', 0.21200985], ['promontory', 0.23786893], ['promontory', 0.23685344], ['promontory', 0.22901785]]\n",
      "[['promontory', 0.21200985], ['promontory', 0.23786893], ['promontory', 0.23685344], ['promontory', 0.22901785], ['promontory', 0.1733238]]\n",
      "[['promontory', 0.23786893], ['promontory', 0.23685344], ['promontory', 0.22901785], ['promontory', 0.1733238], ['lakeside', 0.11941714]]\n",
      "[['promontory', 0.23685344], ['promontory', 0.22901785], ['promontory', 0.1733238], ['lakeside', 0.11941714], ['promontory', 0.15581425]]\n",
      "[['promontory', 0.22901785], ['promontory', 0.1733238], ['lakeside', 0.11941714], ['promontory', 0.15581425], ['lakeside', 0.14158311]]\n",
      "[['promontory', 0.1733238], ['lakeside', 0.11941714], ['promontory', 0.15581425], ['lakeside', 0.14158311], ['lakeside', 0.08394971]]\n",
      "[['lakeside', 0.11941714], ['promontory', 0.15581425], ['lakeside', 0.14158311], ['lakeside', 0.08394971], ['lakeside', 0.097436525]]\n",
      "[['promontory', 0.15581425], ['lakeside', 0.14158311], ['lakeside', 0.08394971], ['lakeside', 0.097436525], ['lakeside', 0.12303327]]\n",
      "[['lakeside', 0.14158311], ['lakeside', 0.08394971], ['lakeside', 0.097436525], ['lakeside', 0.12303327], ['lakeside', 0.099005036]]\n",
      "[['lakeside', 0.08394971], ['lakeside', 0.097436525], ['lakeside', 0.12303327], ['lakeside', 0.099005036], ['lakeside', 0.12998618]]\n",
      "[['lakeside', 0.097436525], ['lakeside', 0.12303327], ['lakeside', 0.099005036], ['lakeside', 0.12998618], ['lakeside', 0.12159471]]\n",
      "[['lakeside', 0.12303327], ['lakeside', 0.099005036], ['lakeside', 0.12998618], ['lakeside', 0.12159471], ['lakeside', 0.10257779]]\n",
      "[['lakeside', 0.099005036], ['lakeside', 0.12998618], ['lakeside', 0.12159471], ['lakeside', 0.10257779], ['lakeside', 0.07082923]]\n",
      "[['lakeside', 0.12998618], ['lakeside', 0.12159471], ['lakeside', 0.10257779], ['lakeside', 0.07082923], ['lakeside', 0.10260719]]\n",
      "[['lakeside', 0.12159471], ['lakeside', 0.10257779], ['lakeside', 0.07082923], ['lakeside', 0.10260719], ['lakeside', 0.093660645]]\n",
      "[['lakeside', 0.10257779], ['lakeside', 0.07082923], ['lakeside', 0.10260719], ['lakeside', 0.093660645], ['lakeside', 0.09727072]]\n",
      "[['lakeside', 0.07082923], ['lakeside', 0.10260719], ['lakeside', 0.093660645], ['lakeside', 0.09727072], ['lakeside', 0.07560347]]\n",
      "[['lakeside', 0.10260719], ['lakeside', 0.093660645], ['lakeside', 0.09727072], ['lakeside', 0.07560347], ['promontory', 0.091537744]]\n",
      "[['lakeside', 0.093660645], ['lakeside', 0.09727072], ['lakeside', 0.07560347], ['promontory', 0.091537744], ['promontory', 0.11349209]]\n",
      "[['lakeside', 0.09727072], ['lakeside', 0.07560347], ['promontory', 0.091537744], ['promontory', 0.11349209], ['lakeside', 0.11095044]]\n",
      "[['lakeside', 0.07560347], ['promontory', 0.091537744], ['promontory', 0.11349209], ['lakeside', 0.11095044], ['lakeside', 0.10293015]]\n",
      "[['promontory', 0.091537744], ['promontory', 0.11349209], ['lakeside', 0.11095044], ['lakeside', 0.10293015], ['lakeside', 0.10166129]]\n",
      "[['promontory', 0.11349209], ['lakeside', 0.11095044], ['lakeside', 0.10293015], ['lakeside', 0.10166129], ['seashore', 0.10159579]]\n",
      "[['lakeside', 0.11095044], ['lakeside', 0.10293015], ['lakeside', 0.10166129], ['seashore', 0.10159579], ['lakeside', 0.08927619]]\n",
      "[['lakeside', 0.10293015], ['lakeside', 0.10166129], ['seashore', 0.10159579], ['lakeside', 0.08927619], ['lakeside', 0.085822985]]\n",
      "[['lakeside', 0.10166129], ['seashore', 0.10159579], ['lakeside', 0.08927619], ['lakeside', 0.085822985], ['lakeside', 0.10461614]]\n",
      "[['seashore', 0.10159579], ['lakeside', 0.08927619], ['lakeside', 0.085822985], ['lakeside', 0.10461614], ['lakeside', 0.11875403]]\n",
      "[['lakeside', 0.08927619], ['lakeside', 0.085822985], ['lakeside', 0.10461614], ['lakeside', 0.11875403], ['lakeside', 0.08048966]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e9e47aac3167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/mila/a/aditya.jain/scratch/selfsupervise_data/natgeo_wild/natgeo_wild_6.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhard_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_LOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d2584083330b>\u001b[0m in \u001b[0;36mhard_examples\u001b[0;34m(video_path, model, save_loc, window, threshold, categories, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mpred_list\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# don't need the first element now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_indx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# setting which frame to get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VID_DATA_PATH = '/home/mila/a/aditya.jain/scratch/selfsupervise_data/sitcom/'\n",
    "VID_FILES     = os.listdir(VID_DATA_PATH)\n",
    "SAVE_LOC      = '/home/mila/a/aditya.jain/scratch/selfsupervise_data/hard_examples_v2/'\n",
    "WINDOW        = 2\n",
    "THRESHOLD     = 0.5\n",
    "\n",
    "for video in VID_FILES:\n",
    "    video_path = VID_DATA_PATH + video\n",
    "    video_path = '/home/mila/a/aditya.jain/scratch/selfsupervise_data/natgeo_wild/natgeo_wild_6.mp4'\n",
    "    print(video_path)\n",
    "    hard_examples(video_path, resnet_mod, SAVE_LOC, WINDOW, THRESHOLD, categories, device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth)",
   "language": "python",
   "name": "milamoth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
