{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author        : Aditya Jain\n",
    "Date Started  : 21st June, 2021\n",
    "About         : This script generates Grad-Class Activation Maps (GradCAM) for lots of images\n",
    "'''\n",
    "import sys, os\n",
    "sys.path.append('/home/mila/a/aditya.jain/mothAI/deeplearning')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from models.resnet50 import Resnet50\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "\n",
    "# Display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from data.mothdataset import MOTHDataset\n",
    "\n",
    "image_path   = 'test2.jpg'\n",
    "THRESHOLD    = 0.25\n",
    "\n",
    "bbox_dir     = '/home/mila/a/aditya.jain/bboximages/correct/'\n",
    "config_file  = '/home/mila/a/aditya.jain/mothAI/deeplearning/config/01-config.json' \n",
    "PATH         = '/home/mila/a/aditya.jain/logs/v01_mothmodel_2021-06-08-04-53.pt'\n",
    "label_file   = '/home/mila/a/aditya.jain/mothAI/deeplearning/data/numeric_labels.json'\n",
    "\n",
    "f             = open(config_file)\n",
    "config_data   = json.load(f)\n",
    "\n",
    "root_dir      = config_data['dataset']['root_dir']\n",
    "test_set      = config_data['dataset']['test_set']\n",
    "label_list    = config_data['dataset']['label_info']\n",
    "batch_size    = 1\n",
    "image_resize  = config_data['training']['image_resize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, image, label_file, predict_indx, last_conv_out):\n",
    "    '''\n",
    "    given an image and model, returns the CAM\n",
    "    '''\n",
    "    f            = open(label_file)\n",
    "    label_info   = json.load(f)\n",
    "    species_list = label_info['species_list']\n",
    "    \n",
    "    last_conv_out = last_conv_out[0]\n",
    "    conv_shape = last_conv_out.shape\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name == 'classifier.weight':\n",
    "            classifier_weights = param \n",
    "            break\n",
    "    \n",
    "    topclass_weights = classifier_weights[predict_indx,:]\n",
    "\n",
    "    cam = np.zeros((conv_shape[1], conv_shape[1]))\n",
    "\n",
    "    for i in range(conv_shape[1]):\n",
    "        for j in range(conv_shape[1]):\n",
    "            for k in range(conv_shape[0]):\n",
    "                cam[i,j] += topclass_weights[k]*last_conv_out[k,i,j]\n",
    "                \n",
    "    cam = np.maximum(cam, 0)/np.amax(cam)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "def cropped_image(cam, threshold, orig_im):\n",
    "    '''\n",
    "    based on CAM, returns the cropped image from the original based on the chosen threshold\n",
    "    '''\n",
    "    heatmap           = cv2.resize(cam, (image_resize, image_resize))\n",
    "    heatmap           = np.uint8(255 * heatmap)\n",
    "    \n",
    "    threshold_val     = threshold*np.amax(heatmap)\n",
    "    threshold_heatmap = (heatmap > threshold_val) * heatmap\n",
    "    \n",
    "    labels, nb        = ndimage.label(threshold_heatmap)\n",
    "    if nb==1:\n",
    "        crop_loc          = ndimage.find_objects(labels==1)\n",
    "        cropped_im        = orig_im[crop_loc[0]]\n",
    "        return cropped_im\n",
    "    else:\n",
    "        return 'Multiple segments found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(cam, image_batch, image_resize, threshold, bbox_dir, count):\n",
    "    '''\n",
    "    prints the bounding box on the original image\n",
    "    '''\n",
    "    orig_im           = np.array(image_batch.squeeze())\n",
    "    orig_im           = np.moveaxis(orig_im, 0, -1)\n",
    "    \n",
    "    heatmap           = cv2.resize(cam, (image_resize, image_resize))\n",
    "    heatmap           = np.uint8(255 * heatmap)    \n",
    "    threshold_val     = threshold*np.amax(heatmap)\n",
    "    threshold_heatmap = (heatmap > threshold_val) * heatmap    \n",
    "    labels, nb        = ndimage.label(threshold_heatmap)\n",
    "    \n",
    "    if nb==1:\n",
    "        crop_loc          = ndimage.find_objects(labels==1)    \n",
    "        y_start           = crop_loc[0][0].start\n",
    "        y_stop            = crop_loc[0][0].stop\n",
    "        x_start           = crop_loc[0][1].start\n",
    "        x_stop            = crop_loc[0][1].stop\n",
    "        \n",
    "        plt.imshow(orig_im)\n",
    "        ax       = plt.gca()\n",
    "        rect     = patches.Rectangle((x_start,y_start),\n",
    "                                     x_stop-x_start,\n",
    "                                     y_stop-y_start,\n",
    "                                     linewidth=3,\n",
    "                                     edgecolor='red',\n",
    "                                     fill = False)\n",
    "        ax.add_patch(rect)\n",
    "        plt.savefig(bbox_dir + str(count) + '.jpg')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print('Multiple segments found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting bboxes for test data images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fa6e3ff1ed0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model         = Resnet50(config_data).to(device)\n",
    "\n",
    "checkpoint  = torch.load(PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# adding hook on the last convolution layer\n",
    "last_conv_output = None\n",
    "\n",
    "def hook(module, input, output):\n",
    "    global last_conv_output\n",
    "    last_conv_output = output    \n",
    "\n",
    "model.backbone[7][2].relu.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[437]]) tensor([[437]])\n",
      "tensor(437)\n",
      "tensor(509)\n",
      "tensor(619)\n",
      "tensor(310)\n",
      "tensor(691)\n"
     ]
    }
   ],
   "source": [
    "# test data loader\n",
    "test_transformer  = transforms.Compose([\n",
    "                        transforms.Resize((image_resize, image_resize)),              # resize the image to 224x224 \n",
    "                        transforms.ToTensor()])\n",
    "test_data         = MOTHDataset(root_dir, test_set, label_list, test_transformer)\n",
    "test_dataloader   = DataLoader(test_data,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "count = 0\n",
    "model.eval()\n",
    "with torch.no_grad():                                 # switching off gradient computation in evaluation mode\n",
    "    for image_batch, label_batch in test_dataloader:  \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        out                      = model(image_batch)\n",
    "        _, predict_indx          = torch.topk(out, 1)\n",
    "        print(label_batch, predict_indx)\n",
    "        \n",
    "        if label_batch in predict_indx:\n",
    "            _, predict_indx          = torch.topk(out, 5)\n",
    "            predict_indx             = predict_indx[0]\n",
    "            \n",
    "            for index in predict_indx:\n",
    "                print(index)\n",
    "                cam                      = generate_cam(model, image_batch, label_file, index, last_conv_output)\n",
    "                bounding_box(cam, image_batch, image_resize, THRESHOLD, bbox_dir, count)        \n",
    "                count +=1 \n",
    "        \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth)",
   "language": "python",
   "name": "milamoth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
