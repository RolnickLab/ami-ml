{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Author        : Aditya Jain\n",
    "Date started  : April 19, 2022\n",
    "About         : Script to test the idea of using CNN for tracking\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from resnet50 import Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resize = 224\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "total_species = 768\n",
    "model         = Resnet50(total_species).to(device)\n",
    "PATH          = '/home/mila/a/aditya.jain/logs/v01_mothmodel_2021-06-08-04-53.pt'\n",
    "checkpoint    = torch.load(PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# print(summary(model, (3,224,224)))  # keras-type model summary\n",
    "\n",
    "# only getting the last feature layer\n",
    "model         = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# print(model)\n",
    "# print(summary(model, (3,224,224)))  # keras-type model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading localization annotation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir     = '/home/mila/a/aditya.jain/scratch/TrapData_QuebecVermont_2022/Vermont/'\n",
    "image_folder = '2022_05_13'\n",
    "\n",
    "image_dir   = data_dir + image_folder + '/'\n",
    "annot_file  = data_dir + 'localize_classify_annotation-' + image_folder + '.json'\n",
    "track_file  = data_dir + 'tracking_annotation-' + image_folder + '.csv'\n",
    "\n",
    "data_images = os.listdir(image_dir)\n",
    "data_annot  = json.load(open(annot_file))\n",
    "\n",
    "track_info  = []    # [<image_name>, <track_id>, <bb_topleft_x>, <bb_topleft_y>, <bb_botright_x>, <bb_botright_y>]\n",
    "track_id    = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image, img_resize):\n",
    "    \"\"\"transforms the cropped moth images for model prediction\"\"\"\n",
    "    \n",
    "    transformer = transforms.Compose([\n",
    "                  transforms.Resize((img_resize, img_resize)),              # resize the image to 224x224 \n",
    "                  transforms.ToTensor()])    \n",
    "    image       = transformer(image)\n",
    "    \n",
    "    # RGBA image; extra alpha channel\n",
    "    if image.shape[0]>3:  # \n",
    "        image  = image[0:3,:,:]\n",
    "        \n",
    "    # grayscale image; converted to 3 channels r=g=b\n",
    "    if image.shape[0]==1: \n",
    "        to_pil    = transforms.ToPILImage()\n",
    "        to_rgb    = transforms.Grayscale(num_output_channels=3)\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        image     = to_tensor(to_rgb(to_pil(image)))\n",
    "        \n",
    "    return image\n",
    "\n",
    "def l1_normalize(v):\n",
    "    norm = np.sum(np.array(v))\n",
    "    return v / norm\n",
    "\n",
    "\n",
    "def save_track(image_dir, data_images, data_annot, idx, model, img_resize, device):\n",
    "    \"\"\"\n",
    "    finds the track between annotations of two consecutive images\n",
    "    \n",
    "    Args:\n",
    "    image_dir (str)    : path to image directory\n",
    "    data_images (list) : list of trap images\n",
    "    data_annot (dict)  : dictionary containing annotation information for each image\n",
    "    idx (int)          : image index for which the track needs to be found\n",
    "    model              : model for finding the cnn features\n",
    "    img_resize (int)   : resizing size\n",
    "    device (str)       : device being used, cuda/cpu            \n",
    "    \"\"\"\n",
    "    \n",
    "    global track_info, track_id, COST_THR\n",
    "    \n",
    "    image1 = cv2.imread(image_dir + data_images[idx-1])\n",
    "    image2 = cv2.imread(image_dir + data_images[idx])\n",
    "    \n",
    "    image1_annot = data_annot[data_images[idx-1]][0]\n",
    "    image2_annot = data_annot[data_images[idx]][0]\n",
    "    \n",
    "    print('Image 1')\n",
    "    plt.figure()\n",
    "    plt.imshow(image1)\n",
    "    \n",
    "    print('Image 2')\n",
    "    plt.figure()\n",
    "    plt.imshow(image2)\n",
    "    \n",
    "    for i in range(len(image2_annot)):\n",
    "        for j in range(len(image1_annot)):\n",
    "            \n",
    "            # getting image2 cropped moth photo\n",
    "            img2_annot  = image2_annot[i]\n",
    "            img2_moth   = image2[img2_annot[1]:img2_annot[3], \\\n",
    "                                 img2_annot[0]:img2_annot[2]]\n",
    "            img2_moth   = Image.fromarray(img2_moth)\n",
    "            img2_moth   = transform_image(img2_moth, img_resize)\n",
    "#             plt.figure()\n",
    "#             plt.imshow(np.transpose(img2_moth))\n",
    "            img2_moth   = torch.unsqueeze(img2_moth, 0).to(device)\n",
    "            \n",
    "            # getting image1 cropped moth photo\n",
    "            img1_annot  = image1_annot[j]\n",
    "            img1_moth   = image1[img1_annot[1]:img1_annot[3], \\\n",
    "                                 img1_annot[0]:img1_annot[2]]\n",
    "            img1_moth   = Image.fromarray(img1_moth)\n",
    "            img1_moth   = transform_image(img1_moth, img_resize)\n",
    "#             plt.figure()\n",
    "#             plt.imshow(np.transpose(img1_moth))\n",
    "            img1_moth   = torch.unsqueeze(img1_moth, 0).to(device)\n",
    "            \n",
    "            # getting model features for each image\n",
    "            with torch.no_grad():\n",
    "                img2_ftrs   = model(img2_moth)\n",
    "                img2_ftrs   = img2_ftrs.view(-1, img2_ftrs.size(0)).cpu()\n",
    "                img2_ftrs   = img2_ftrs.reshape((img2_ftrs.shape[0], ))\n",
    "                img2_ftrs   = l1_normalize(img2_ftrs)\n",
    "            \n",
    "                img1_ftrs   = model(img1_moth)\n",
    "                img1_ftrs   = img1_ftrs.view(-1, img1_ftrs.size(0)).cpu()\n",
    "                img1_ftrs   = img1_ftrs.reshape((img1_ftrs.shape[0], ))\n",
    "                img1_ftrs   = l1_normalize(img1_ftrs)\n",
    "            \n",
    "            # find cosine similarity\n",
    "            cosine_sim  = np.dot(img1_ftrs, img2_ftrs)/(np.linalg.norm(img1_ftrs)*np.linalg.norm(img2_ftrs))\n",
    "            euclid_dist = np.linalg.norm(img2_ftrs-img1_ftrs)\n",
    "            \n",
    "            print('Cosine similarity: ', cosine_sim)\n",
    "            print('Euclidean distance: ', euclid_dist)\n",
    "            \n",
    "#             break\n",
    "#             print('Image 2 features: ', img2_ftrs, img2_ftrs.shape)\n",
    "#             print('Image 1 features: ', img1_ftrs, img1_ftrs.shape)\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the tracking annotation for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_annot = data_annot[data_images[0]][0]\n",
    "\n",
    "for i in range(len(first_annot)):\n",
    "    track_info.append([data_images[0], track_id, \n",
    "                       first_annot[i][0], first_annot[i][1], \n",
    "                       first_annot[i][2], first_annot[i][3],\n",
    "                       first_annot[i][0] + int((first_annot[i][2]-first_annot[i][0])/2),\n",
    "                       first_annot[i][1] + int((first_annot[i][3]-first_annot[i][1])/2)])\n",
    "    track_id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the tracking annotation for the rest images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500, len(data_images)):\n",
    "    save_track(image_dir, data_images, data_annot, i, \\\n",
    "               model, image_resize, device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth)",
   "language": "python",
   "name": "milamoth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
